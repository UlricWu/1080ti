{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 106,
=======
   "execution_count": 1,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## random generate numbers\n",
    "n = 10000\n",
    "d = 10\n",
    "x = np.random.normal(size=(n,d))\n",
    "error = np.random.normal(size=(n,1))\n",
    "\n",
    "b = np.random.uniform(size=(d,1))\n",
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(500,4))\n",
    "error = np.random.normal(size=(500,1))\n",
    "b = np.random.uniform(size=(4,1))\n",
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
    "\n",
    "### in order to avoid Perfect Separation, we need to add some noise\n",
    "y = x@b + error\n",
    "\n",
<<<<<<< HEAD
    "# y = list(map(lambda x: 1 if x > 0 else 0, y))\n",
    "# y = np.array(y)\n",
    "\n",
    "# binary\n",
    "y = np.array([0 if x <0 else 1  for x in y]).reshape(-1,1)\n",
    "\n",
    "## 3 classes\n",
    "# y = np.array([0 if x < -1 else 1 if x < 1 else 2 for x in y]).reshape(-1,1)\n"
=======
    "y = list(map(lambda x: 1 if x > 0 else 0, y))\n",
    "y = np.array(y).reshape(-1,1)"
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 108,
=======
   "execution_count": 3,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generate_process:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y    \n",
    "        \n",
    "    def split(self, rate = 0.7, random_state = 1024, scale = False):\n",
    "        ## Feature scaling is used to normalize the range of independent variables or features of data\n",
    "        if scale:\n",
    "            self.x = (self.x - np.mean(self.x))/x.std()\n",
    "        \n",
    "        n = len(self.y)\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        ##randomly spilte data into 70% train and 30% test\n",
    "        index = list(range(n))\n",
    "        np.random.shuffle(index)\n",
    "        train = index[:int(rate*n)]\n",
    "        test = index[int(rate*n):]\n",
    "        \n",
    "        self.train_x = self.x[train]\n",
    "        self.test_x = self.x[test]\n",
    "        self.train_y = self.y[train]\n",
    "        self.test_y = self.y[test]\n",
    "        \n",
    "        return self.train_x, self.test_x, self.train_y, self.test_y\n",
    "    \n",
    "class model:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "     \n",
    "    \n",
    "    def bias(self, intercept):\n",
    "        ## if need intercept, assign x0 as 1\n",
    "        if intercept:\n",
    "            n = len(self.x)\n",
    "            ones = np.ones((n,1))\n",
    "            return np.hstack([ones,self.x])\n",
    "        return self.x\n",
<<<<<<< HEAD
    "    def label(self, prob = None, ):\n",
    "        \n",
    "        labels = list(map(lambda x: 1 if x > self.threshold else 0, prob))\n",
=======
    "    def label(self, prob = None, threshold = 0.5):\n",
    "        \n",
    "        labels = list(map(lambda x: 1 if x > threshold else 0, prob))\n",
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
    "        return np.array(labels).reshape(-1,1)\n",
    "        \n",
    "    def tidy(self, x, tails = 2):\n",
    "        \n",
    "        n, k = x.shape\n",
    "        self.error = self.y - x@self.beta \n",
    "        if not self.vb.any():\n",
    "            self.vb = self.error.var()*np.linalg.inv(x.T@x)\n",
    "            \n",
    "        self.se = np.sqrt(np.diagonal(self.vb)).reshape(-1,1)\n",
    "            \n",
    "        self.t = np.divide(self.beta,self.se)\n",
    "        self.pval = tails * (1 - stats.norm.cdf(self.t))\n",
    "        \n",
    "        names = ['Coef','Std err','t','p-value']\n",
    "        values = [self.beta, self.se, self.t, self.pval]\n",
    "        values = np.hstack(values)\n",
    "        \n",
    "        self.summary = pd.DataFrame(values, columns =names)\n",
    "\n",
    "        self.rsq = 1 - self.error.var()/self.y.var()\n",
    "        self.adjrsq = self.rsq*(n -1)/(n-k-1)\n",
    "        \n",
    "        var = self.error.var()\n",
    "        ## sum function here is useless, but just in order to get a list, rather than a list in a list\n",
    "        ## in this case, it's easily to plot graph\n",
    "        self.sse = sum(self.error.T@self.error)\n",
    "        \n",
    "        if not len(self.logl):\n",
    "            logl = -n/2*np.log(2*var*math.pi) - 1/2/var*self.sse\n",
    "        \n",
    "#         self.logl = self.logl.tolist()\n",
    "        \n",
    "        self.aic = -2*(self.logl)+ 2*k \n",
    "        self.bic = -2*(self.logl)+ k*np.log(n) \n",
    "        \n",
    "        names = ['r.squared','adj.rsq','df','loglikehood','aic','bic']\n",
    "        values = [self.rsq,self.adjrsq,n-k-1,self.logl,self.aic,self.bic]\n",
    "        glance= pd.DataFrame(columns = ['r.squared','adj.rsq','df','loglikehood','aic','bic'])\n",
    "        glance.loc[0] = values\n",
    "        \n",
    "        self.glance = glance \n",
    "        return  self.summary\n",
    "    \n",
    "    def performance(self, test_y, y_pred):\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        tp = 0\n",
    "\n",
    "        for i in range(len(test_y)):\n",
    "\n",
    "            if y_pred[i] == test_y[i]:\n",
    "                if y_pred[i] == 1:\n",
    "                    tp += 1\n",
    "                    continue\n",
    "\n",
    "                tn += 1\n",
    "                continue\n",
    "\n",
    "            if y_pred[i] == 1:\n",
    "                    fp += 1\n",
    "                    continue \n",
    "            fn += 1\n",
    "\n",
    "        self.tn, self.fp, self.fn, self.tp = tn, fp, fn, tp\n",
    "\n",
    "        self.confusion = pd.DataFrame({'predict 0':[tn, fn], 'predict 1':[fp, tp]})\n",
    "        \n",
    "        self.accuray = (self.tn + self.tp)/len(test_y)\n",
    "        \n",
    "        self.precion = tp/(tp + fp)\n",
    "        self.recall = tp/(tp + fn)\n",
    "        self.fmeasure = (2 * self.precion * self.recall) / (self.recall + self.precion) \n",
    "    \n",
    "        return {'': self.confusion,\n",
    "            'Accuracy': self.accuray,\n",
    "            'Recall': self.recall,\n",
    "            'Precion': self.precion,\n",
    "            'Fmeasure': self.fmeasure \n",
    "             }\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, x, beta):\n",
    "        return 1/(1+np.exp(-x@beta))\n",
    "    \n",
    "    def Hession(self, x, beta):\n",
    "        \n",
    "        sigmoid = self.sigmoid(x, beta)\n",
    "        \n",
    "        w = np.diagflat(sigmoid*(1-sigmoid))\n",
    "        return x.T@w@x\n",
    "    \n",
    "    def Jacobian(self, x, beta):\n",
    "        \n",
    "        sigmoid = self.sigmoid(x, beta)\n",
    "        \n",
    "        return x.T@(self.y- sigmoid)\n",
    "        \n",
<<<<<<< HEAD
    "    def LogisticRegression(self, algorithm = 'MLE', alpha = 0.001, iterations = int(1e+5), threshold = 1e-3, \\\n",
=======
    "    def LogisticRegression(self, algorithm = 'MLE', alpha = 0.001,iterations = int(1e+6), threshold = 1e-5, \\\n",
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
    "                           intercept = None, random_state = None):\n",
    "        \n",
    "        x = self.bias(intercept)\n",
    "        m, n = x.shape\n",
    "        \n",
    "        if random_state is None:\n",
    "             ## init beta = [1,1,1]\n",
    "            beta = np.ones(n).reshape(-1,1)  \n",
    "        else:\n",
    "                ## random start\n",
    "            beta = np.random.randn(n).reshape(-1,1)\n",
    "\n",
    "        cost = []\n",
    "\n",
    "        if algorithm == 'MLE':\n",
    "            \n",
    "            for i in range(iterations):\n",
    "                update =  np.linalg.solve(self.Hession(x, beta), self.Jacobian(x, beta))   \n",
    "                beta += update\n",
    "\n",
    "                ## threshold measure if learning step is accuracy\n",
    "                if np.abs(update).sum()< threshold:\n",
    "                    self.beta = beta\n",
    "                    self.vb = np.linalg.inv(self.Hession(x, beta))\n",
    "                    self.logl = self.y.T@x@beta - sum(np.log(1+np.exp(x@beta)))\n",
    "                    return self.tidy(x)\n",
    "                \n",
    "        if algorithm == 'GradientDescent':\n",
    "            for i in range(iterations):\n",
<<<<<<< HEAD
    "                update =  alpha*x.T@(1/(1+np.exp(-x@beta)) - self.y)\n",
=======
    "                update =  alpha/m*x.T@(1/(1+np.exp(-x@beta)) - self.y)\n",
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
    "                beta -= update\n",
    "\n",
    "                h = 1/np.exp(-x@beta)\n",
    "                cost.append(sum((h - self.y).T@(h - self.y)/len(self.y)))\n",
    "                if (i > 100) and abs(cost[-1] - cost[len(cost)-2]) <= 10e-5:\n",
    "                    self.beta = beta\n",
    "                    self.cost = cost\n",
    "                    return beta\n",
    "            \n",
    "        return Exception('Gradient did not converge')\n",
    "    \n",
    "\n",
    "    \n",
<<<<<<< HEAD
    "    def predict(self, model, x, beta = None, threshold = 0.5):\n",
    "        self.threshold = threshold\n",
=======
    "    def predict(self, model, x, beta = None):\n",
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
    "        if not beta:\n",
    "            beta = self.beta\n",
    "            \n",
    "        if model == 'LogisticRegression':\n",
    "            self.prob = self.sigmoid(x, beta)\n",
    "            return self.label(self.prob)\n",
    "        \n",
    "        if model == 'LinearRegression':\n",
    "            return x@bta\n",
    "        \n",
    "        return Exception('Model is not Supportable')\n",
    "        \n",
    "\n",
    "\n",
    "train_x, test_x, train_y, test_y = data_generate_process(x, y ).split()"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "\n",
    "$$\\mathcal L=\\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}$$\n",
    "$$\\mathcal L =\\prod_{i=1}^n F(x_i'\\beta)^{y_i}(1-F(x_i'\\beta))^{1-y_i}$$\n",
    "$$\\ln\\mathcal L=\\sum_{i=1}^n y_i \\ln{F(x_i'\\beta)}+(1-y_i)\\ln{(1-F(x_i'\\beta))}$$\n",
    "$$\\ln\\mathcal L=\\left[\\ln{F(\\beta'\\mathbf X')}\\right]y+\\left[\\ln{(\\mathbf{1}'-F(\\beta'\\mathbf X'))}\\right](1-y)$$\n",
    "\n",
    "#### Newtown Method\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\text{diag}\\left(\\frac{f(\\mathbf X\\beta)}{F(\\mathbf X\\beta)}\\right)y-\\mathbf X'\\text{diag}\\left(\\frac{f(\\mathbf X\\beta)}{\\mathbf 1-F(\\mathbf X\\beta)}\\right)(1-y)$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\text{diag}\\left(\\frac{f(\\mathbf X\\beta)(1-F(\\mathbf X\\beta))}{(1-F(\\mathbf X\\beta))F(\\mathbf X\\beta)}\\right)y-\\mathbf X'\\text{diag}\\left(\\frac{f(\\mathbf X\\beta)F(\\mathbf X\\beta)}{\\mathbf F(\\mathbf X\\beta)(1-F(\\mathbf X\\beta))}\\right)(1-y)$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\left[\\text{diag}\\left(1-F(\\mathbf X\\beta)\\right)y-\\text{diag}\\left(F(\\mathbf X\\beta)\\right)(1-y)\\right]$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\left[\\text{diag}\\left(y-F(\\mathbf X\\beta)y-F(\\mathbf X\\beta)+F(\\mathbf X\\beta)y)\\right)\\right]\\mathbf 1$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\left[\\text{diag}\\left(y-F(\\mathbf X\\beta)\\right)\\right]\\mathbf 1$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\left[y-F(\\mathbf X\\beta)\\right]$$\n",
    "\n",
    "#### Hessian\n",
    "$$\\frac{d}{d\\beta}\\frac{d\\ln\\mathcal L}{d\\beta}'=\\frac{d}{d\\beta}\\left[y'-F(\\beta'\\mathbf X')\\right]\\mathbf X$$\n",
    "$$\\frac{d^2\\ln\\mathcal L}{d\\beta d\\beta'}=-\\mathbf X'\\left[\\text{diag}\\left(f(\\mathbf X\\beta)\\right)\\right]\\mathbf X$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
=======
   "cell_type": "code",
   "execution_count": 4,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef</th>\n",
       "      <th>Std err</th>\n",
       "      <th>t</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
<<<<<<< HEAD
       "      <td>1.587345</td>\n",
       "      <td>0.052174</td>\n",
       "      <td>30.424012</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.533409</td>\n",
       "      <td>0.051025</td>\n",
       "      <td>30.051931</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.287496</td>\n",
       "      <td>0.039094</td>\n",
       "      <td>7.354032</td>\n",
       "      <td>1.922906e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.211075</td>\n",
       "      <td>0.046263</td>\n",
       "      <td>26.177856</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.875472</td>\n",
       "      <td>0.041976</td>\n",
       "      <td>20.856302</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.960136</td>\n",
       "      <td>0.043396</td>\n",
       "      <td>22.125142</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.829918</td>\n",
       "      <td>0.041501</td>\n",
       "      <td>19.997492</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.438080</td>\n",
       "      <td>0.049408</td>\n",
       "      <td>29.106386</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.437571</td>\n",
       "      <td>0.049921</td>\n",
       "      <td>28.797051</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.594259</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>14.871401</td>\n",
       "      <td>0.000000e+00</td>\n",
=======
       "      <td>0.458605</td>\n",
       "      <td>0.139727</td>\n",
       "      <td>3.282139</td>\n",
       "      <td>1.030228e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.240656</td>\n",
       "      <td>0.169700</td>\n",
       "      <td>7.310860</td>\n",
       "      <td>2.653433e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.546233</td>\n",
       "      <td>0.135461</td>\n",
       "      <td>4.032404</td>\n",
       "      <td>5.520915e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.237859</td>\n",
       "      <td>0.166907</td>\n",
       "      <td>7.416462</td>\n",
       "      <td>1.203482e-13</td>\n",
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
<<<<<<< HEAD
       "       Coef   Std err          t       p-value\n",
       "0  1.587345  0.052174  30.424012  0.000000e+00\n",
       "1  1.533409  0.051025  30.051931  0.000000e+00\n",
       "2  0.287496  0.039094   7.354032  1.922906e-13\n",
       "3  1.211075  0.046263  26.177856  0.000000e+00\n",
       "4  0.875472  0.041976  20.856302  0.000000e+00\n",
       "5  0.960136  0.043396  22.125142  0.000000e+00\n",
       "6  0.829918  0.041501  19.997492  0.000000e+00\n",
       "7  1.438080  0.049408  29.106386  0.000000e+00\n",
       "8  1.437571  0.049921  28.797051  0.000000e+00\n",
       "9  0.594259  0.039960  14.871401  0.000000e+00"
      ]
     },
     "execution_count": 109,
=======
       "       Coef   Std err         t       p-value\n",
       "0  0.458605  0.139727  3.282139  1.030228e-03\n",
       "1  1.240656  0.169700  7.310860  2.653433e-13\n",
       "2  0.546233  0.135461  4.032404  5.520915e-05\n",
       "3  1.237859  0.166907  7.416462  1.203482e-13"
      ]
     },
     "execution_count": 4,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## default use MaxLikehood Estimation\n",
    "logit = model(train_x, train_y)\n",
    "logit.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 110,
=======
   "execution_count": 5,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r.squared</th>\n",
       "      <th>adj.rsq</th>\n",
       "      <th>df</th>\n",
       "      <th>loglikehood</th>\n",
       "      <th>aic</th>\n",
       "      <th>bic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
<<<<<<< HEAD
       "      <td>-41.41472</td>\n",
       "      <td>-41.473977</td>\n",
       "      <td>6989</td>\n",
       "      <td>[[-2206.1160653918723]]</td>\n",
       "      <td>[[4432.232130783745]]</td>\n",
       "      <td>[[4500.76878506412]]</td>\n",
=======
       "      <td>-10.834829</td>\n",
       "      <td>-10.96045</td>\n",
       "      <td>345</td>\n",
       "      <td>[[-162.30998659562013]]</td>\n",
       "      <td>[[332.61997319124026]]</td>\n",
       "      <td>[[348.0517058091741]]</td>\n",
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
<<<<<<< HEAD
       "   r.squared    adj.rsq    df              loglikehood                    aic  \\\n",
       "0  -41.41472 -41.473977  6989  [[-2206.1160653918723]]  [[4432.232130783745]]   \n",
       "\n",
       "                    bic  \n",
       "0  [[4500.76878506412]]  "
      ]
     },
     "execution_count": 110,
=======
       "   r.squared   adj.rsq   df              loglikehood                     aic  \\\n",
       "0 -10.834829 -10.96045  345  [[-162.30998659562013]]  [[332.61997319124026]]   \n",
       "\n",
       "                     bic  \n",
       "0  [[348.0517058091741]]  "
      ]
     },
     "execution_count": 5,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.glance"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 111,
=======
   "execution_count": 6,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
<<<<<<< HEAD
      "         Current function value: 0.315159\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                 7000\n",
      "Model:                          Logit   Df Residuals:                     6990\n",
      "Method:                           MLE   Df Model:                            9\n",
      "Date:                Wed, 15 Jul 2020   Pseudo R-squ.:                  0.5453\n",
      "Time:                        21:01:38   Log-Likelihood:                -2206.1\n",
      "converged:                       True   LL-Null:                       -4852.0\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             1.5873      0.052     30.424      0.000       1.485       1.690\n",
      "x2             1.5334      0.051     30.052      0.000       1.433       1.633\n",
      "x3             0.2875      0.039      7.354      0.000       0.211       0.364\n",
      "x4             1.2111      0.046     26.178      0.000       1.120       1.302\n",
      "x5             0.8755      0.042     20.856      0.000       0.793       0.958\n",
      "x6             0.9601      0.043     22.125      0.000       0.875       1.045\n",
      "x7             0.8299      0.042     19.997      0.000       0.749       0.911\n",
      "x8             1.4381      0.049     29.106      0.000       1.341       1.535\n",
      "x9             1.4376      0.050     28.797      0.000       1.340       1.535\n",
      "x10            0.5943      0.040     14.871      0.000       0.516       0.673\n",
=======
      "         Current function value: 0.463743\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  350\n",
      "Model:                          Logit   Df Residuals:                      346\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Sat, 11 Apr 2020   Pseudo R-squ.:                  0.3307\n",
      "Time:                        17:31:50   Log-Likelihood:                -162.31\n",
      "converged:                       True   LL-Null:                       -242.51\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.502e-34\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.4586      0.140      3.282      0.001       0.185       0.732\n",
      "x2             1.2407      0.170      7.311      0.000       0.908       1.573\n",
      "x3             0.5462      0.135      4.032      0.000       0.281       0.812\n",
      "x4             1.2379      0.167      7.416      0.000       0.911       1.565\n",
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "gamma_model = sm.Logit(train_y, train_x)\n",
    "\n",
    "gamma_results = gamma_model.fit()\n",
    "\n",
    "print(gamma_results.summary())\n",
    "\n",
    "## same as the package "
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Gradient Descent\n",
    "$$\\mathcal L= y\\log(\\hat y)+(1-y)\\log(1-\\hat y)$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta} = LearningRate*(\\hat y - y)x$$\n",
    "$$\\hat y = sigmod(-XBeta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
=======
   "cell_type": "code",
   "execution_count": 7,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "array([[1.58734545],\n",
       "       [1.53340897],\n",
       "       [0.28749574],\n",
       "       [1.21107538],\n",
       "       [0.87547162],\n",
       "       [0.96013601],\n",
       "       [0.82991766],\n",
       "       [1.43808028],\n",
       "       [1.43757065],\n",
       "       [0.59425866]])"
      ]
     },
     "execution_count": 112,
=======
       "array([[ 0.2949109 ],\n",
       "       [ 0.60422571],\n",
       "       [-0.10776678],\n",
       "       [ 1.39051323]])"
      ]
     },
     "execution_count": 7,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.LogisticRegression(random_state = True, algorithm = 'GradientDescent')\n",
    "## it's not atble but you can get more accuracy over multiple time"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 113,
=======
   "execution_count": 8,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "array([[1.58734545],\n",
       "       [1.53340897],\n",
       "       [0.28749574],\n",
       "       [1.21107538],\n",
       "       [0.87547162],\n",
       "       [0.96013601],\n",
       "       [0.82991766],\n",
       "       [1.43808028],\n",
       "       [1.43757065],\n",
       "       [0.59425866]])"
      ]
     },
     "execution_count": 113,
=======
       "array([[0.98570343],\n",
       "       [1.00825686],\n",
       "       [0.98560841],\n",
       "       [1.00731386]])"
      ]
     },
     "execution_count": 8,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.LogisticRegression( 'GradientDescent')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.57478021],\n",
       "       [1.52151705],\n",
       "       [0.28507075],\n",
       "       [1.20180097],\n",
       "       [0.86895746],\n",
       "       [0.95281951],\n",
       "       [0.8237596 ],\n",
       "       [1.42698924],\n",
       "       [1.42617673],\n",
       "       [0.58991366]])"
      ]
     },
     "execution_count": 114,
=======
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fz_ws\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.44729587, 1.19061913, 0.53001824, 1.19109257]])"
      ]
     },
     "execution_count": 9,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(train_x, train_y.ravel())\n",
<<<<<<< HEAD
    "clf.coef_.reshape(-1, 1)\n",
=======
    "clf.coef_\n",
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
    "\n",
    "## sklearn estimates coefficients."
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction comparison\n",
    "#### adjust threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
=======
   "cell_type": "code",
   "execution_count": 10,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'':    predict 0  predict 1\n",
<<<<<<< HEAD
       " 0       1326        194\n",
       " 1        216       1264,\n",
       " 'Accuracy': 0.8633333333333333,\n",
       " 'Recall': 0.8540540540540541,\n",
       " 'Precion': 0.8669410150891632,\n",
       " 'Fmeasure': 0.8604492852280463}"
      ]
     },
     "execution_count": 118,
=======
       " 0         47         19\n",
       " 1         25         59,\n",
       " 'Accuracy': 0.7066666666666667,\n",
       " 'Recall': 0.7023809523809523,\n",
       " 'Precion': 0.7564102564102564,\n",
       " 'Fmeasure': 0.728395061728395}"
      ]
     },
     "execution_count": 10,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "# GradientDescent\n",
    "logit.LogisticRegression( 'GradientDescent')\n",
    "y_pred = logit.predict('LogisticRegression', test_x, threshold = 0.5) \n",
=======
    "logit.LogisticRegression( 'GradientDescent')\n",
    "y_pred = logit.predict('LogisticRegression', test_x) \n",
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
    "logit.performance(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 119,
=======
   "execution_count": 11,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'':    predict 0  predict 1\n",
<<<<<<< HEAD
       " 0       1326        194\n",
       " 1        216       1264,\n",
       " 'Accuracy': 0.8633333333333333,\n",
       " 'Recall': 0.8540540540540541,\n",
       " 'Precion': 0.8669410150891632,\n",
       " 'Fmeasure': 0.8604492852280463}"
      ]
     },
     "execution_count": 119,
=======
       " 0         46         20\n",
       " 1         23         61,\n",
       " 'Accuracy': 0.7133333333333334,\n",
       " 'Recall': 0.7261904761904762,\n",
       " 'Precion': 0.7530864197530864,\n",
       " 'Fmeasure': 0.7393939393939394}"
      ]
     },
     "execution_count": 11,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "# MLE\n",
    "logit.LogisticRegression()\n",
    "y_pred = logit.predict('LogisticRegression', test_x, threshold = 0.5) \n",
=======
    "logit.LogisticRegression()\n",
    "y_pred = logit.predict('LogisticRegression', test_x) \n",
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
    "logit.performance(test_y, y_pred)\n",
    "\n",
    "## slice different from MLE and gradient descent"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 120,
=======
   "execution_count": 12,
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
<<<<<<< HEAD
      "           0       0.86      0.87      0.87      1520\n",
      "           1       0.87      0.85      0.86      1480\n",
      "\n",
      "    accuracy                           0.86      3000\n",
      "   macro avg       0.86      0.86      0.86      3000\n",
      "weighted avg       0.86      0.86      0.86      3000\n",
      "\n",
      "confusion_matrix \n",
      " [[1324  196]\n",
      " [ 216 1264]]\n"
=======
      "           0       0.67      0.67      0.67        66\n",
      "           1       0.74      0.74      0.74        84\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       150\n",
      "   macro avg       0.70      0.70      0.70       150\n",
      "weighted avg       0.71      0.71      0.71       150\n",
      "\n",
      "confusion_matrix \n",
      " [[44 22]\n",
      " [22 62]]\n"
>>>>>>> 27dc72bf56eba6ef0435dd9625fa62998278179c
     ]
    }
   ],
   "source": [
    "print( classification_report(test_y, clf.predict(test_x)))\n",
    "print('confusion_matrix \\n',confusion_matrix(test_y, clf.predict(test_x)))\n",
    "## sklearn is not as good as expected in this case, but it really depedences on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
