{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(500,4))\n",
    "error = np.random.normal(size=(500,1))\n",
    "b = np.random.uniform(size=(4,1))\n",
    "\n",
    "### in order to avoid Perfect Separation, we need to add some noise\n",
    "y = x@b + error\n",
    "\n",
    "y = list(map(lambda x: 1 if x > 0 else 0, y))\n",
    "y = np.array(y).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generate_process:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y    \n",
    "        \n",
    "    def split(self, rate = 0.7, random_state = 1024, scale = False):\n",
    "        ## Feature scaling is used to normalize the range of independent variables or features of data\n",
    "        if scale:\n",
    "            self.x = (self.x - np.mean(self.x))/x.std()\n",
    "        \n",
    "        n = len(self.y)\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        ##randomly spilte data into 70% train and 30% test\n",
    "        index = list(range(n))\n",
    "        np.random.shuffle(index)\n",
    "        train = index[:int(rate*n)]\n",
    "        test = index[int(rate*n):]\n",
    "        \n",
    "        self.train_x = self.x[train]\n",
    "        self.test_x = self.x[test]\n",
    "        self.train_y = self.y[train]\n",
    "        self.test_y = self.y[test]\n",
    "        \n",
    "        return self.train_x, self.test_x, self.train_y, self.test_y\n",
    "    \n",
    "class model:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "     \n",
    "    \n",
    "    def bias(self, intercept):\n",
    "        ## if need intercept, assign x0 as 1\n",
    "        if intercept:\n",
    "            n = len(self.x)\n",
    "            ones = np.ones((n,1))\n",
    "            return np.hstack([ones,self.x])\n",
    "        return self.x\n",
    "    def label(self, prob = None, threshold = 0.5):\n",
    "        \n",
    "        labels = list(map(lambda x: 1 if x > threshold else 0, prob))\n",
    "        return np.array(labels).reshape(-1,1)\n",
    "        \n",
    "    def tidy(self, x, tails = 2):\n",
    "        \n",
    "        n, k = x.shape\n",
    "        self.error = self.y - x@self.beta \n",
    "        if not self.vb.any():\n",
    "            self.vb = self.error.var()*np.linalg.inv(x.T@x)\n",
    "            \n",
    "        self.se = np.sqrt(np.diagonal(self.vb)).reshape(-1,1)\n",
    "            \n",
    "        self.t = np.divide(self.beta,self.se)\n",
    "        self.pval = tails * (1 - stats.norm.cdf(self.t))\n",
    "        \n",
    "        names = ['Coef','Std err','t','p-value']\n",
    "        values = [self.beta, self.se, self.t, self.pval]\n",
    "        values = np.hstack(values)\n",
    "        \n",
    "        self.summary = pd.DataFrame(values, columns =names)\n",
    "\n",
    "        self.rsq = 1 - self.error.var()/self.y.var()\n",
    "        self.adjrsq = self.rsq*(n -1)/(n-k-1)\n",
    "        \n",
    "        var = self.error.var()\n",
    "        ## sum function here is useless, but just in order to get a list, rather than a list in a list\n",
    "        ## in this case, it's easily to plot graph\n",
    "        self.sse = sum(self.error.T@self.error)\n",
    "        \n",
    "        if not len(self.logl):\n",
    "            logl = -n/2*np.log(2*var*math.pi) - 1/2/var*self.sse\n",
    "        \n",
    "#         self.logl = self.logl.tolist()\n",
    "        \n",
    "        self.aic = -2*(self.logl)+ 2*k \n",
    "        self.bic = -2*(self.logl)+ k*np.log(n) \n",
    "        \n",
    "        names = ['r.squared','adj.rsq','df','loglikehood','aic','bic']\n",
    "        values = [self.rsq,self.adjrsq,n-k-1,self.logl,self.aic,self.bic]\n",
    "        glance= pd.DataFrame(columns = ['r.squared','adj.rsq','df','loglikehood','aic','bic'])\n",
    "        glance.loc[0] = values\n",
    "        \n",
    "        self.glance = glance \n",
    "        return  self.summary\n",
    "    \n",
    "    def performance(self, test_y, y_pred):\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        tp = 0\n",
    "\n",
    "        for i in range(len(test_y)):\n",
    "\n",
    "            if y_pred[i] == test_y[i]:\n",
    "                if y_pred[i] == 1:\n",
    "                    tp += 1\n",
    "                    continue\n",
    "\n",
    "                tn += 1\n",
    "                continue\n",
    "\n",
    "            if y_pred[i] == 1:\n",
    "                    fp += 1\n",
    "                    continue \n",
    "            fn += 1\n",
    "\n",
    "        self.tn, self.fp, self.fn, self.tp = tn, fp, fn, tp\n",
    "\n",
    "        self.confusion = pd.DataFrame({'predict 0':[tn, fn], 'predict 1':[fp, tp]})\n",
    "        \n",
    "        self.accuray = (self.tn + self.tp)/len(test_y)\n",
    "        \n",
    "        self.precion = tp/(tp + fp)\n",
    "        self.recall = tp/(tp + fn)\n",
    "        self.fmeasure = (2 * self.precion * self.recall) / (self.recall + self.precion) \n",
    "    \n",
    "        return {'': self.confusion,\n",
    "            'Accuracy': self.accuray,\n",
    "            'Recall': self.recall,\n",
    "            'Precion': self.precion,\n",
    "            'Fmeasure': self.fmeasure \n",
    "             }\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, x, beta):\n",
    "        return 1/(1+np.exp(-x@beta))\n",
    "    \n",
    "    def Hession(self, x, beta):\n",
    "        \n",
    "        sigmoid = self.sigmoid(x, beta)\n",
    "        \n",
    "        w = np.diagflat(sigmoid*(1-sigmoid))\n",
    "        return x.T@w@x\n",
    "    \n",
    "    def Jacobian(self, x, beta):\n",
    "        \n",
    "        sigmoid = self.sigmoid(x, beta)\n",
    "        \n",
    "        return x.T@(self.y- sigmoid)\n",
    "        \n",
    "    def LogisticRegression(self, algorithm = 'MLE', alpha = 0.001,iterations = int(1e+6), threshold = 1e-5, \\\n",
    "                           intercept = None, random_state = None):\n",
    "        \n",
    "        x = self.bias(intercept)\n",
    "        m, n = x.shape\n",
    "        \n",
    "        if random_state is None:\n",
    "             ## init beta = [1,1,1]\n",
    "            beta = np.ones(n).reshape(-1,1)  \n",
    "        else:\n",
    "                ## random start\n",
    "            beta = np.random.randn(n).reshape(-1,1)\n",
    "\n",
    "        cost = []\n",
    "\n",
    "        if algorithm == 'MLE':\n",
    "            \n",
    "            for i in range(iterations):\n",
    "                update =  np.linalg.solve(self.Hession(x, beta), self.Jacobian(x, beta))   \n",
    "                beta += update\n",
    "\n",
    "                ## threshold measure if learning step is accuracy\n",
    "                if np.abs(update).sum()< threshold:\n",
    "                    self.beta = beta\n",
    "                    self.vb = np.linalg.inv(self.Hession(x, beta))\n",
    "                    self.logl = self.y.T@x@beta - sum(np.log(1+np.exp(x@beta)))\n",
    "                    return self.tidy(x)\n",
    "                \n",
    "        if algorithm == 'GradientDescent':\n",
    "            for i in range(iterations):\n",
    "                update =  alpha/m*x.T@(1/(1+np.exp(-x@beta)) - self.y)\n",
    "                beta -= update\n",
    "\n",
    "                h = 1/np.exp(-x@beta)\n",
    "                cost.append(sum((h - self.y).T@(h - self.y)/len(self.y)))\n",
    "                if (i > 100) and abs(cost[-1] - cost[len(cost)-2]) <= 10e-5:\n",
    "                    self.beta = beta\n",
    "                    self.cost = cost\n",
    "                    return beta\n",
    "            \n",
    "        return Exception('Gradient did not converge')\n",
    "    \n",
    "\n",
    "    \n",
    "    def predict(self, model, x, beta = None):\n",
    "        if not beta:\n",
    "            beta = self.beta\n",
    "            \n",
    "        if model == 'LogisticRegression':\n",
    "            self.prob = self.sigmoid(x, beta)\n",
    "            return self.label(self.prob)\n",
    "        \n",
    "        if model == 'LinearRegression':\n",
    "            return x@bta\n",
    "        \n",
    "        return Exception('Model is not Supportable')\n",
    "        \n",
    "\n",
    "\n",
    "train_x, test_x, train_y, test_y = data_generate_process(x, y ).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef</th>\n",
       "      <th>Std err</th>\n",
       "      <th>t</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.458605</td>\n",
       "      <td>0.139727</td>\n",
       "      <td>3.282139</td>\n",
       "      <td>1.030228e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.240656</td>\n",
       "      <td>0.169700</td>\n",
       "      <td>7.310860</td>\n",
       "      <td>2.653433e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.546233</td>\n",
       "      <td>0.135461</td>\n",
       "      <td>4.032404</td>\n",
       "      <td>5.520915e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.237859</td>\n",
       "      <td>0.166907</td>\n",
       "      <td>7.416462</td>\n",
       "      <td>1.203482e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Coef   Std err         t       p-value\n",
       "0  0.458605  0.139727  3.282139  1.030228e-03\n",
       "1  1.240656  0.169700  7.310860  2.653433e-13\n",
       "2  0.546233  0.135461  4.032404  5.520915e-05\n",
       "3  1.237859  0.166907  7.416462  1.203482e-13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## default use MaxLikehood Estimation\n",
    "logit = model(train_x, train_y)\n",
    "logit.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r.squared</th>\n",
       "      <th>adj.rsq</th>\n",
       "      <th>df</th>\n",
       "      <th>loglikehood</th>\n",
       "      <th>aic</th>\n",
       "      <th>bic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10.834829</td>\n",
       "      <td>-10.96045</td>\n",
       "      <td>345</td>\n",
       "      <td>[[-162.30998659562013]]</td>\n",
       "      <td>[[332.61997319124026]]</td>\n",
       "      <td>[[348.0517058091741]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   r.squared   adj.rsq   df              loglikehood                     aic  \\\n",
       "0 -10.834829 -10.96045  345  [[-162.30998659562013]]  [[332.61997319124026]]   \n",
       "\n",
       "                     bic  \n",
       "0  [[348.0517058091741]]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.463743\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  350\n",
      "Model:                          Logit   Df Residuals:                      346\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Sat, 11 Apr 2020   Pseudo R-squ.:                  0.3307\n",
      "Time:                        17:31:50   Log-Likelihood:                -162.31\n",
      "converged:                       True   LL-Null:                       -242.51\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.502e-34\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.4586      0.140      3.282      0.001       0.185       0.732\n",
      "x2             1.2407      0.170      7.311      0.000       0.908       1.573\n",
      "x3             0.5462      0.135      4.032      0.000       0.281       0.812\n",
      "x4             1.2379      0.167      7.416      0.000       0.911       1.565\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "gamma_model = sm.Logit(train_y, train_x)\n",
    "\n",
    "gamma_results = gamma_model.fit()\n",
    "\n",
    "print(gamma_results.summary())\n",
    "\n",
    "## same as the package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2949109 ],\n",
       "       [ 0.60422571],\n",
       "       [-0.10776678],\n",
       "       [ 1.39051323]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.LogisticRegression(random_state = True, algorithm = 'GradientDescent')\n",
    "## it's not atble but you can get more accuracy over multiple time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98570343],\n",
       "       [1.00825686],\n",
       "       [0.98560841],\n",
       "       [1.00731386]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.LogisticRegression( 'GradientDescent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fz_ws\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.44729587, 1.19061913, 0.53001824, 1.19109257]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(train_x, train_y.ravel())\n",
    "clf.coef_\n",
    "\n",
    "## sklearn estimates coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'':    predict 0  predict 1\n",
       " 0         47         19\n",
       " 1         25         59,\n",
       " 'Accuracy': 0.7066666666666667,\n",
       " 'Recall': 0.7023809523809523,\n",
       " 'Precion': 0.7564102564102564,\n",
       " 'Fmeasure': 0.728395061728395}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.LogisticRegression( 'GradientDescent')\n",
    "y_pred = logit.predict('LogisticRegression', test_x) \n",
    "logit.performance(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'':    predict 0  predict 1\n",
       " 0         46         20\n",
       " 1         23         61,\n",
       " 'Accuracy': 0.7133333333333334,\n",
       " 'Recall': 0.7261904761904762,\n",
       " 'Precion': 0.7530864197530864,\n",
       " 'Fmeasure': 0.7393939393939394}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.LogisticRegression()\n",
    "y_pred = logit.predict('LogisticRegression', test_x) \n",
    "logit.performance(test_y, y_pred)\n",
    "\n",
    "## slice different from MLE and gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67        66\n",
      "           1       0.74      0.74      0.74        84\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       150\n",
      "   macro avg       0.70      0.70      0.70       150\n",
      "weighted avg       0.71      0.71      0.71       150\n",
      "\n",
      "confusion_matrix \n",
      " [[44 22]\n",
      " [22 62]]\n"
     ]
    }
   ],
   "source": [
    "print( classification_report(test_y, clf.predict(test_x)))\n",
    "print('confusion_matrix \\n',confusion_matrix(test_y, clf.predict(test_x)))\n",
    "## sklearn is not as good as expected in this case, but it really depedences on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
